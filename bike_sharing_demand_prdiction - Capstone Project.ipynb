{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1leECdCeDnLJ7nReHtQSOV7irVKTMsNMi","timestamp":1684763788354},{"file_id":"1pLHuk5ml8brfq76USc0n0suZU4FLxCqd","timestamp":1684763648790}],"collapsed_sections":["vncDsAP0Gaoa","FJNUwmbgGyua","w6K7xa23Elo4","yQaldy8SH6Dl","PH-0ReGfmX4f","mDgbUHAGgjLW","O_i_v8NEhb9l","HhfV-JJviCcP","Y3lxredqlCYt","3RnN4peoiCZX","x71ZqKXriCWQ","7hBIi_osiCS2","JlHwYmJAmNHm","35m5QtbWiB9F","PoPl-ycgm1ru","H0kj-8xxnORC","nA9Y7ga8ng1Z","PBTbrJXOngz2","u3PMJOP6ngxN","dauF4eBmngu3","bKJF3rekwFvQ","MSa1f5Uengrz","GF8Ens_Soomf","0wOQAZs5pc--","K5QZ13OEpz2H","lQ7QKXXCp7Bj","448CDAPjqfQr","KSlN3yHqYklG","t6dVpIINYklI","ijmpgYnKYklI","-JiQyfWJYklI","EM7whBJCYoAo","fge-S5ZAYoAp","85gYPyotYoAp","RoGjAbkUYoAp","4Of9eVA-YrdM","iky9q4vBYrdO","F6T5p64dYrdO","y-Ehk30pYrdP","bamQiAODYuh1","QHF8YVU7Yuh3","GwzvFGzlYuh3","qYpmQ266Yuh3","OH-pJp9IphqM","bbFf2-_FphqN","_ouA3fa0phqN","Seke61FWphqN","PIIx-8_IphqN","t27r6nlMphqO","r2jJGEOYphqO","b0JNsNcRphqO","BZR9WyysphqO","jj7wYXLtphqO","eZrbJ2SmphqO","rFu4xreNphqO","YJ55k-q6phqO","gCFgpxoyphqP","OVtJsKN_phqQ","lssrdh5qphqQ","U2RJ9gkRphqQ","1M8mcRywphqQ","tgIPom80phqQ","JMzcOPDDphqR","x-EpHcCOp1ci","X_VqEhTip1ck","8zGJKyg5p1ck","PVzmfK_Ep1ck","n3dbpmDWp1ck","ylSl6qgtp1ck","ZWILFDl5p1ck","M7G43BXep1ck","Ag9LCva-p1cl","E6MkPsBcp1cl","2cELzS2fp1cl","3MPXvC8up1cl","NC_X3p0fY2L0","UV0SzAkaZNRQ","YPEH6qLeZNRQ","q29F0dvdveiT","EXh0U9oCveiU","22aHeOlLveiV","JcMwzZxoAimU","8G2x9gOozGDZ","gCX9965dhzqZ","gIfDvo9L0UH2"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<h1><b>Project Type - Supervised Machine learning (Regression Model)\n","<h1><b>Contribution - Individual"],"metadata":{"id":"BWEbylDQoVu1"}},{"cell_type":"markdown","source":["<h1><b>GitHUb Link\n","\n"],"metadata":{"id":"qZfOHsMQp4A2"}},{"cell_type":"markdown","source":["<h1><b>Project Summary\n","\n","* This project aimed to predict how many bikes would be rented at different times based on factors like weather and environmental conditions. The dataset used had information on temperature, humidity, wind speed, and other factors.\n","\n","* The dataset used in the study consisted of 8760 entries with 14 features.\n","\n","* To start, the dataset was checked for any missing values, outliers, or relationships between the features. Categorical variables were transformed into numerical ones. The dataset was then divided into a training set (70%) and a testing set (30%).\n","\n","* The first step was to clean and explore the data. There were no duplicate or empty values, and the columns were converted to the right data types. Exploratory analysis was done to find any patterns or trends in the data.\n","\n","* Next, seven different machine learning algorithms were trained and evaluated: Linear Regression, Polynomial Regression, Ridge Regression, Lasso Regression, Elastic Net Regression, Decision Tree, Random Forest, and XGBoost Regression. Their performance was assessed using metrics like mean squared error (MSE), root mean squared error (RMSE), R2 score, and mean absolute error (MAE). The best models were further fine-tuned using hyperparameter tuning.\n","\n","* The results showed that the Random Forest and XGBoost models performed better than the other models, with R2 scores of 0.89 and 0.91, respectively. These models were found to be the most accurate in predicting the demand for rental bikes.\n","\n","* In conclusion, this study demonstrated that machine learning algorithms can accurately predict the demand for rental bikes. These findings can assist bike rental companies in making better decisions and improving their services to meet the growing demand."],"metadata":{"id":"06tPVaYmpWDz"}},{"cell_type":"markdown","source":["<h1><b>Business Context\n","\n","* The bike rental service in Seoul, known as the capital bike share system, allows people to rent bikes by the hour or day. It's important for this service to accurately predict how many bikes will be rented at different times. This helps them plan their operations effectively, reduce waiting times, and make sure they have enough bikes available for customers.\n","\n","* To make these predictions, they can use machine learning models based on historical data. By analyzing past rental patterns and factors like time of day, weather, and other relevant information, the model can estimate how many bikes will be rented in the future. This information is crucial for the rental service to adjust their bike supply accordingly, ensuring that customers don't have to wait for bikes and have a good experience.\n","\n","* Accurate prediction of bike rental demand is very beneficial. It allows the rental service to plan better, provide a more efficient service, and ultimately satisfy their customers. It also helps improve overall mobility in the city and can result in increased revenue for the bike rental service. By having a competitive advantage in predicting demand, the bike rental service can stay ahead in the market and offer a more reliable and convenient service to the public"],"metadata":{"id":"bHXw7x7bqSlC"}},{"cell_type":"markdown","source":["<h1><b>Problem Statement\n","\n","\n","The city of Seoul has a bike-sharing system where people can rent bikes. They have collected data on how many bikes are rented, as well as information about the weather and the time of year, for the years 2017 and 2018. -\n","\n","\n","* The goal of this project is to create a machine learning model that can accurately predict how many bikes will be rented in Seoul based on this historical data. The model will take into account factors like the weather conditions, the season (such as summer or winter), and the time of day. By considering all these factors, the model will be able to make more accurate predictions about how many bikes will be needed at any given time.\n","\n","* Having this model will be very helpful for the city's bike rental service. It will allow them to better plan and manage their bike supply, making sure they have enough bikes available for people to rent. This will improve the overall customer experience by reducing waiting times and ensuring that people can easily find a bike when they need one. By accurately predicting the demand for rental bikes, the city's bike rental service can provide a more reliable and convenient service to its residents and visitors."],"metadata":{"id":"h405bNlupGel"}},{"cell_type":"markdown","source":["# ***Let's Begin !***"],"metadata":{"id":"O_i_v8NEhb9l"}},{"cell_type":"markdown","source":["## ***1. Know Your Data***"],"metadata":{"id":"HhfV-JJviCcP"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"0LeUrsYMYkJz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Import Libraries"],"metadata":{"id":"Y3lxredqlCYt"}},{"cell_type":"code","source":["# Import Libraries\n","# Importing Pandas and Numpy\n","import pandas as pd\n","import numpy as np\n","from numpy import math\n","\n","# importing visualization libraries\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sns\n","from seaborn.rcmod import set_style\n","\n","import datetime as dt\n","from datetime import datetime\n","\n","# Importing Models libraries\n","from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.ensemble import RandomForestRegressor\n","import xgboost as xgb\n","from xgboost.sklearn import XGBRegressor\n","\n","from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n","from sklearn.model_selection import cross_validate, cross_val_score\n","\n","# import evaluation metrics\n","from sklearn import metrics\n","from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n","\n","# Importing warning for ignore warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n"],"metadata":{"id":"M8Vqi-pPk-HR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Loading"],"metadata":{"id":"3RnN4peoiCZX"}},{"cell_type":"code","source":["# Load Dataset\n","df = pd.read_csv('/content/drive/MyDrive/capstone project 2/SeoulBikeData.csv',encoding=\"latin1\")"],"metadata":{"id":"4CkvbW_SlZ_R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# making a copy of data for safity purpose\n","df_copy = df.copy()\n"],"metadata":{"id":"h2vuV09pZ3TV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset First View"],"metadata":{"id":"x71ZqKXriCWQ"}},{"cell_type":"code","source":["# Dataset First Look\n","df.head()"],"metadata":{"id":"LWNFOSvLl09H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Rows & Columns count"],"metadata":{"id":"7hBIi_osiCS2"}},{"cell_type":"code","source":["# Dataset Rows & Columns count\n","df.shape"],"metadata":{"id":"Kllu7SJgmLij"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Information"],"metadata":{"id":"JlHwYmJAmNHm"}},{"cell_type":"code","source":["# Dataset Info\n","df.info()"],"metadata":{"id":"e9hRXRi6meOf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Duplicate Values"],"metadata":{"id":"35m5QtbWiB9F"}},{"cell_type":"code","source":["# Dataset Duplicate Value Count\n","print(len(df[df.duplicated()]))\n"],"metadata":{"id":"1sLdpKYkmox0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Missing Values/Null Values"],"metadata":{"id":"PoPl-ycgm1ru"}},{"cell_type":"code","source":["# Missing Values/Null Values Count\n","df.isna().sum().sort_values(ascending= False).reset_index().rename(columns={'index':'Columns',0:'Null values'})"],"metadata":{"id":"GgHWkxvamxVg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizing the missing values\n","plt.figure(figsize=(14, 5))\n","sns.heatmap(df.isnull(), cbar=True, yticklabels=False)\n","plt.xlabel(\"column_name\", size=14, weight=\"bold\")\n","plt.title(\"Missing Values in Column\",fontweight=\"bold\",size=17)\n","plt.show()"],"metadata":{"id":"3q5wnI3om9sJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Describe the dataset\n","df.describe(include='all').T"],"metadata":{"id":"pBZRc8hFb35M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dataset Columns\n","\n","# fetch attribute\n","df.columns"],"metadata":{"id":"j7xfkqrt5Ag5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Variables Description"],"metadata":{"id":"PBTbrJXOngz2"}},{"cell_type":"markdown","source":["> Date - year-month-day\n","\n",">Rented Bike count - Count of bikes rented at each hour\n","\n",">Hour - Hour of the day\n","\n",">Temperature - Temperature in Celsius\n","\n",">Humidity - %\n","\n",">Windspeed - m/s\n","\n",">Visibility - 10m\n","\n",">Dew point temperature - Celsius\n","\n",">Solar radiation - MJ/m2\n","\n",">Rainfall - mm\n","\n",">Snowfall - cm\n","\n",">Seasons - Winter, Spring, Summer, Autumn\n","\n",">Holiday - Holiday/No holiday\n","\n",">Functional Day - NoFunc(Non Functional Hours), Fun(Functional hours)"],"metadata":{"id":"aJV4KIxSnxay"}},{"cell_type":"markdown","source":["### Check Unique Values for each variable."],"metadata":{"id":"u3PMJOP6ngxN"}},{"cell_type":"code","source":["# Check Unique Values for each variable.\n","df.nunique().reset_index().rename(columns={'index':'Columns',0:'Unique values'})"],"metadata":{"id":"zms12Yq5n-jE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h1>understandng the more about your variable<h>\n"],"metadata":{"id":"fpbGrnLLs0nq"}},{"cell_type":"markdown","source":["> The dataset is from a rental bike company based out of Seoul. The goal of this project is to develop a machine learning model that can predict the demand for rental bikes.\n","\n","> There are not any null and duplicate value in the dataset.\n","\n","> Dataset has 8760 entries with 14 features.\n","\n","> The dataset contains the hourly weather conditions for a period of 364\n","  days, and other details such as whether a said day was a holiday or not."],"metadata":{"id":"Yt4Es-lot6DT"}},{"cell_type":"markdown","source":["## 3. ***Data Wrangling***\n","\n","<B/>  Data wrangling, also known as data cleaning or data preprocessing, is the\n","  process of transforming and cleaning raw data into a structured and usable\n","  format. It involves various tasks such as removing irrelevant or duplicate\n","  data, and converting data into a standardized format. The goal of data\n","  wrangling is to make the data more suitable for analysis, which involves\n","  making it accurate, complete, and consistent."],"metadata":{"id":"dauF4eBmngu3"}},{"cell_type":"markdown","source":["<b><h2>Renaming the column name :-<b/><h/>\n","\n","<h6>To improve the readability of a dataset and enhance understanding is to rename the column names."],"metadata":{"id":"kI0BhkwT0Ybx"}},{"cell_type":"code","source":["# Write your code to make your dataset analysis ready.\n","\n","# renaming the features\n","\n","df.rename(columns= {'Date':'date','Rented Bike Count': 'rented_bike_count', 'Hour':'hour',\n","                    'Temperature(°C)':'temperature', 'Humidity(%)':'humidity',\n","                    'Wind speed (m/s)': 'wind_speed', 'Visibility (10m)': 'visibility',\n","                    'Dew point temperature(°C)':'dew_point_temp',\n","                    'Solar Radiation (MJ/m2)': 'solar_radiation', 'Rainfall(mm)': 'rainfall',\n","                    'Snowfall (cm)':'snowfall', 'Seasons':'seasons',\n","                    'Holiday':'holiday', 'Functioning Day':'func_day'},\n","          inplace=True)"],"metadata":{"id":"wk-9a2fpoLcV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h3>Converting the date column in appropriate format"],"metadata":{"id":"9X3gGkqb1qo_"}},{"cell_type":"code","source":["# finding the datatype of 'Date' column\n","\n","type(df['date'][0])"],"metadata":{"id":"abOUCgjc1od7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# converting string format of 'Date' column into date-time format\n","\n","df['date'] = pd.to_datetime(df['date'])"],"metadata":{"id":"8lAmpaaR16EF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"EewNiSu116RP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h4>Convert the \"Date\" column into 3 different column i.e \"Year\",\"Month\" and \"Day\" :\n","\n",">\"Year\" column contains the 2 unique numbers, details from 2017 december to\n"," 2018 november.So if i consider this as a year then we don't need this column, so we can drop it.\n","\n",">\"Day\" column contains the details about the each day of the month, considering\n","  day wise data is too long, so we concise this data into a day is a weekend. Therfore, convert it into this format and drop the \"day\" column."],"metadata":{"id":"NZs6oT7t2aHY"}},{"cell_type":"code","source":["\n","# extracting day,month, day of week and weekdays/weekend from date column\n","\n","df['month'] = df['date'].dt.month\n","df['day_of_week'] = df['date'].dt.day_name()\n","\n","#Converted weekdays and weekend into binary class as Weekdays = 0 and Weekend = 1.\n","df['weekdays_weekend']=df['day_of_week'].apply(lambda x : \"1\" if x=='Saturday' or x=='Sunday' else \"0\" )"],"metadata":{"id":"LVhnX8b22e9v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove date, day_of_week column from data set\n","df=df.drop(columns=['date','day_of_week'],axis=1)"],"metadata":{"id":"1xS_6xvj3sEU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.info()"],"metadata":{"id":"LBXqR56R3tAB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.describe()"],"metadata":{"id":"brHpYsk-30Uj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h3>Look at the data, Hour and month Columns are a numerical columns but these are time stamp so we have to treat Hour and month columns as a categorical feature"],"metadata":{"id":"Cb2km7Nl49ny"}},{"cell_type":"code","source":["\n","# convert Hour column integer to Categorical\n","df['hour']=df['hour'].astype('object')\n","\n","# convert month column integer to Categorical\n","df['month']=df['month'].astype('object')"],"metadata":{"id":"bKdDTrm_5Dcm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's Separate the data into appropriate format:\n","\n","<H4> what is categorical data ?\n","\n","> A categorical variable is a variable that represents a set of categories or\n","  groups. Categorical variables can take on a limited number of possible values, such as yes or no, red, blue or green. Categorical variables are often used to represent qualitative data and are non-numeric in nature. It includes data type such as object and other category.\n","\n","\n","<h4> what is numerical data ?\n","\n","> A numerical variable is a variable that represents numerical values.Numerical\n","  variables can take on any numeric value and can be either discrete or continuous. Examples of numerical variables include age, weight, height, temperature, and income. Numerical variables are often used to represent quantitative data and can be used in mathematical calculations.\n","\n","\n","\n","  <B><h1> Let's extract the numerical and categorical data"],"metadata":{"id":"YifFMSq75LnM"}},{"cell_type":"code","source":["# Divide Data in categorical and numerical features\n","num_col= df.select_dtypes(exclude='object')\n","cat_col=df.select_dtypes(include='object')\n"],"metadata":{"id":"41w5Ol3k5G--"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# fetch unique value in categorical feature columns\n","\n","for col in cat_col:\n","  print('{} has {} values'.format(col,df[col].unique()))\n","  print('\\n')"],"metadata":{"id":"dbK2_fcM6gs0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h2><b>Visualized Categorical column"],"metadata":{"id":"nqzurv-C6hTT"}},{"cell_type":"code","source":["# Set the figure size\n","plt.figure(figsize=(12, 6), dpi=200)\n","\n","# Iterate through each categorical column and create a pie chart\n","for i, feature in enumerate(cat_col):\n","\n","  # Create a subplot for each pie chart\n","  plt.subplot(2,3, i+1)\n","\n","  # Generate the value counts of the current column\n","  value_counts = df[feature].value_counts()\n","\n","  # Generate the pie chart with the value counts of the current column\n","  plt.pie(value_counts, labels=value_counts.index, autopct='%.0f%%')\n","\n","  # Set the title of the subplot to the name of the current column\n","  plt.title(feature, fontsize=12, color='red')\n","\n","  # Adjust the layout of the subplots for better spacing\n","  plt.tight_layout()\n","\n","# Show the pie charts\n","plt.show()"],"metadata":{"id":"S0EsBMgO6ozE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h3><b> Why did you pick this specific chart?\n","\n",">A pie chart is a useful tool to display the distribution of various categories in a dataset. By dividing the circle into proportional sections, each representing a different category, the pie chart allows for a clear comparison of the relative size of each category. The use of different colors for each section further enhances the clarity of the representation and makes it easier to understand and interpret the data."],"metadata":{"id":"i3UBi4uc6tFP"}},{"cell_type":"markdown","source":["<h3><b>What is/are the insight(s) found from the chart?\n","\n",">The data shows that a hours and seasons are equally distributed.\n","\n",">Mostlty bike rented on function_day and non_holidays."],"metadata":{"id":"KYVbwMDf64JZ"}},{"cell_type":"markdown","source":["<h3><b>Will the gained insights help creating a positive business impact?\n","\n",">Yes, gaining insights into the distribution of bike rentals across different times of day and seasons, as well as the preference for functional days over non-holidays, can help bike rental businesses create a positive impact on their operations and customer experience. With this knowledge, bike rental businesses can adjust their resources and marketing strategies to match the demand, ensuring they have enough bikes available when customers need them most."],"metadata":{"id":"kQEq3Xb17FlD"}},{"cell_type":"markdown","source":["## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***\n","\n","<h3>What is EDA :-<h3>\n","                   Exploratory Data Analysis (EDA) is a process of examining data to uncover patterns, relationships, anomalies, and other insights. It involves visualizing and summarizing data, identifying missing data and outliers, and exploring relationships between variables. EDA is a crucial step in the data analysis process and helps to guide further analysis and model selection.\n","\n","\n","\n","\n","<B><h3>Let's start the EDA to find the data insight\n","\n"],"metadata":{"id":"GF8Ens_Soomf"}},{"cell_type":"markdown","source":["#### Chart - 1  <h4> Firstly we start the Analyzing the distribution of the dependent variable"],"metadata":{"id":"0wOQAZs5pc--"}},{"cell_type":"code","source":["# Chart - 1 visualization code# defining dependent variable separately\n","\n","dependent_variable = ['rented_bike_count']"],"metadata":{"id":"7v_ESjsspbW7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualizing the distribution of the dependent variable - rental bike count\n","plt.figure(figsize=(10,5))\n","sns.distplot(df['rented_bike_count'],color=\"c\")\n","plt.title('Distribution of the dependent variable',fontsize=16,color='indigo');\n"],"metadata":{"id":"GAv7t6jP-MnT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#skew of the dependant varaible\n","df[dependent_variable].skew()"],"metadata":{"id":"3LjMV66w-Mq2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df[dependent_variable]"],"metadata":{"id":"EJncOZSHFrPQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tranformed_dep_var=np.sqrt(df[dependent_variable])\n","tranformed_dep_var"],"metadata":{"id":"dYHdeyXsDJRj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualizing the distribution of dependent variable after sqrt transformation\n","plt.figure(figsize=(10,5))\n","sns.distplot(tranformed_dep_var,color=\"c\")\n","plt.title('Distribution of the dependent variable after sqrt',fontsize=16,color='red');"],"metadata":{"id":"2G6zqqpf_YyL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# skew of the dependent variable after sqrt transformation\n","np.sqrt(df[dependent_variable]).skew()\n"],"metadata":{"id":"w1-7tVKUHt34"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?\n","\n","\n","> A displot is a type of chart used to visualize the distribution of a single\n","  variable. It combines a histogram with a kernel density plot to provide an\n","  estimate of the probability density function of the variable. The histogram\n","  displays the frequency distribution of the variable, while the kernel density\n","  plot displays the continuous distribution curve of the variable.The displot\n","  is useful for checking normality, exploring the shape of the distribution,\n","  and identifying any issues that need to be addressed before data analysis."],"metadata":{"id":"K5QZ13OEpz2H"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?\n","\n","\n",">The Rented Bike Count variable is highly skewed towards the right, which violates the normal distribution assumption of linear regression. To address this issue, we applied a data transformation technique to normalize the variable. Specifically, we used the square root transformation, which resulted in a nearly normal distribution for the Rented Bike Count variable."],"metadata":{"id":"lQ7QKXXCp7Bj"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason.\n","\n","\n",">Yes, gaining insights from data analysis can have a positive impact on a business. By analyzing data, businesses can gain a better understanding of customer behavior, market trends, and operational efficiency."],"metadata":{"id":"448CDAPjqfQr"}},{"cell_type":"markdown","source":["#### Chart - 2\n","\n","\n","<h2>Explore relation between categorical feature and dependent variable"],"metadata":{"id":"KSlN3yHqYklG"}},{"cell_type":"code","source":["# Chart - 2 visualization code\n","\n","\n","# fetch categorical columns\n","cat_col"],"metadata":{"id":"R4YgtaqtYklH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Count of Rented bikes acording to Functioning Day\n","\n","plt.figure(figsize=(15,5),dpi=200)\n","sns.pointplot(data=df,x='hour',y='rented_bike_count',hue='func_day')\n","plt.title('Count of Rented bikes acording to Functioning Day', fontsize=16,color='red');\n"],"metadata":{"id":"zqQz3InDJXxy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Count of Rented bikes acording to season\n","\n","plt.figure(figsize=(15,5),dpi=200)\n","sns.pointplot(data=df,x='hour',y='rented_bike_count',hue='seasons')\n","plt.title('Count of Rented bikes acording to seasons',fontsize=16,color='red')"],"metadata":{"id":"L_l86OP4JX1K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Count of Rented bikes acording to Holiday\n","\n","plt.figure(figsize=(15,5),dpi=200)\n","sns.pointplot(data=df,x='hour',y='rented_bike_count',hue='holiday')\n","plt.title('Count of Rented bikes acording to Holiday',fontsize=16,color='red');"],"metadata":{"id":"ZNJI9jXaJYDm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?\n","\n","> A point plot is a type of data visualization in which data points are represented as discrete points along an axis. our aim to display the relationship between numerical variable(rented_bike_count) and categorical variables(holiday,seasons, and functioning day).\n","\n"],"metadata":{"id":"t6dVpIINYklI"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?\n","\n",">Understanding the patterns of bike rentals can be a valuable asset for businesses looking to enhance their operations and customer satisfaction. By analyzing data, businesses can gain insights into the demand for bikes during different seasons, peak hours, and holidays, allowing them to optimize inventory, plan promotions, and improve their services accordingly. By leveraging these insights, businesses can make informed decisions that drive positive business impact and improve the overall customer experience."],"metadata":{"id":"ijmpgYnKYklI"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason.\n","\n"],"metadata":{"id":"-JiQyfWJYklI"}},{"cell_type":"markdown","source":["Understanding the patterns of bike rentals can be a valuable asset for businesses looking to enhance their operations and customer satisfaction. By analyzing data, businesses can gain insights into the demand for bikes during different seasons, peak hours, and holidays, allowing them to optimize inventory, plan promotions, and improve their services accordingly. By leveraging these insights, businesses can make informed decisions that drive positive business impact and improve the overall customer experience.bold textAnswer Here"],"metadata":{"id":"BcBbebzrYklV"}},{"cell_type":"markdown","source":["#### Chart - 3\n","\n","<h2>Explore relation between numerical feature and dependent variable"],"metadata":{"id":"EM7whBJCYoAo"}},{"cell_type":"code","source":["# Chart - 3 visualization code\n","\n","# fetch numerical columns\n","num_col"],"metadata":{"id":"t6GMdE67YoAp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Analyzing the relationship between \"Bike_Count\" and \"Temperature\" :\n","plt.figure(figsize=(15,5),dpi=200)\n","df.groupby('temperature').mean()['rented_bike_count'].plot()\n","plt.title(\"Bike_Count  v/s  temp\",fontsize=16,color='red')\n","plt.ylabel('Bike_count',fontsize=12)\n","plt.xlabel('temp',fontsize=12);"],"metadata":{"id":"lqq1Zh2FLuo0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Analyzing the relationship between \"Bike_Count\" and \"Snowfall\" :\n","plt.figure(figsize=(15,5),dpi=200)\n","df.groupby('snowfall').mean()['rented_bike_count'].plot()\n","plt.title(\"Bike_Count  v/s  Snowfall\",fontsize=16,color='red')\n","plt.ylabel('Bike_count',fontsize=12)\n","plt.xlabel('Snow',fontsize=12);"],"metadata":{"id":"1A5HkdAyLusa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Analyzing the relationship between \"Bike_Count\" and \"Wind\" :\n","plt.figure(figsize=(15,8),dpi=200)\n","df.groupby('wind_speed').mean()['rented_bike_count'].plot()\n","plt.title(\"Bike_Count  v/s  Wind\",fontsize=16,color='red')\n","plt.ylabel('Bike_count',fontsize=12)\n","plt.xlabel('Wind',fontsize=12);"],"metadata":{"id":"UN16ueFtLu76"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Analyzing the relationship between \"Bike_Count\" and \"Rainfall\" :\n","plt.figure(figsize=(15,8),dpi=200)\n","df.groupby('rainfall').mean()['rented_bike_count'].plot()\n","plt.title(\"Bike_Count  v/s  Rainfall\",fontsize=16,color='red')\n","plt.ylabel('Bike_count',fontsize=12)\n","plt.xlabel('Rainfall',fontsize=12);"],"metadata":{"id":"ZMDHfXiGL86I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?\n","\n",">A line plot is a type of plot that displays data as a series of points connected by straight lines. Line plots are commonly used to display the relationship between two numerical variables, where one variable is plotted on the x-axis and the other variable is plotted on the y-axis. Line plots are useful for showing trends or patterns in data over time, and for identifying changes or discontinuities in the data.Answer Here."],"metadata":{"id":"fge-S5ZAYoAp"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?\n","\n","\n",">Answer HereThe count of rented bikes is highest when the temperature is around 30°C, indicating that people prefer to ride bikes in warmer weather. While the count of rented bikes is generally lower in winter compared to other seasons, heavy snowfall (beyond 4cm) leads to a significant drop in bike rentals. Therefore, Snowfall appears to be a major factor influencing the count of rented bikes. Despite a slight drop in rented bikes during moderate rainfall (10-20 mm) the demand for rented bikes does not decrease significantly. In fact, there is a significant increase in rented bikes when there is heavy rainfall (20mm). Thus, it can be concluded that rainfall does not have a major impact on the count of rented bikes."],"metadata":{"id":"85gYPyotYoAp"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason.\n","\n","\n",">Yes, the gained insights from the analysis of the data can help create a positive business impact. For example, the insight that the count of rented bikes is highest when the temperature is around 30°C can be used to adjust the number of bikes available during the warmer months to meet the increased demand. Similarly, the insight that heavy snowfall leads to a significant drop in bike rentals can help the business plan accordingly for the winter months. By taking these insights into consideration, the business can optimize its operations and potentially increase revenue.Answer Here"],"metadata":{"id":"RoGjAbkUYoAp"}},{"cell_type":"markdown","source":["#### Chart - 4\n","\n","<h2>Regression plot to know relation between dependent variable and numerical feature"],"metadata":{"id":"4Of9eVA-YrdM"}},{"cell_type":"code","source":["# Chart - 4 visualization code\n","\n","n=1\n","plt.figure(figsize=(15,10))\n","for i in num_col.columns:\n","  if i == 'rented_bike_count':\n","    pass\n","  else:\n","    plt.subplot(4,2,n)\n","    n+=1\n","    sns.regplot(x=df[i], y=df['rented_bike_count'],scatter_kws={\"color\": \"c\"}, line_kws={\"color\": \"red\"})\n","    plt.title(f'Dependent variable and {i}')\n","    plt.tight_layout()\n","\n","\n"],"metadata":{"id":"irlUoxc8YrdO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h2><b>Why did you pick the specific chart?\n","\n","<H4>A regression plot is a type of data visualization that displays the relationship between two variables using a scatter plot with a regression line. It is commonly used to examine the relationship between a dependent variable and one or more independent variables, and can also be used to identify outliers, trends, and patterns in the data."],"metadata":{"id":"IuNXXRoRkTgg"}},{"cell_type":"markdown","source":["<h2><b>What is/are the insight(s) found from the chart?\n","\n","> <span>1. Positive linear related features :- Hour, Temperature, Wind speed, visibility and solar radiation.\n","\n","> <span>2. Negatively linear related features :- Rainfall, Snowfall, Humidity.\n","\n"],"metadata":{"id":"mbSX0n04keo5"}},{"cell_type":"markdown","source":[" <h2><B>Will the gained insights help creating a positive business impact?\n","\n"," <H4>Yes, the insights gained from the linear regression analysis can be used to create a positive business impact. By identifying the factors that are positively correlated with the count of rented bikes, the business can allocate its resources accordingly, such as increasing the number of bikes available during peak hours and optimizing the rental prices. On the other hand, the insights gained from the factors that have a negative correlation with the count of rented bikes can help the business plan for adverse weather conditions, such as heavy rainfall or snowfall"],"metadata":{"id":"n-qM6XdqmPJu"}},{"cell_type":"markdown","source":["<H1><B> Pair Plot"],"metadata":{"id":"KoB0DSM2m1JI"}},{"cell_type":"code","source":["# Plot pairwise relationships in the dataset\n","sns.pairplot(df, corner=True)"],"metadata":{"id":"BeIS68d2m8ut"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<H3>Why did you pick the specific chart?\n","\n",">Pairplot is a useful tool for visualizing patterns and relationships between variables in a dataset.\n","\n",">In a pairplot, each variable is plotted against all other variables in the dataset, resulting in a grid of scatterplots.\n","\n",">The diagonal of the grid shows the distribution of each variable, while the lower diagonal shows the scatterplots of the pairwise relationships between the variables. The upper diagonal is a mirror image of the lower diagonal, showing the same scatterplots, but with the axes flipped.\n","\n",">Pairplots are useful for identifying patterns and relationships between variables, such as linear or nonlinear relationships, clusters, and outliers. They can also be used to identify which variables are strongly correlated with each other."],"metadata":{"id":"hVXxdvMOnCm6"}},{"cell_type":"markdown","source":["<H3> What is/are the insight(s) found from the chart?\n","\n",">The resulting plot will show scatterplots of each pair of features along with the distributions of each individual feature."],"metadata":{"id":"5Y8Dsv1MnqCu"}},{"cell_type":"markdown","source":[" <H3>Will the gained insights help creating a positive business impact?\n","\n"," >The insights gained from a pair plot can definitely help create a positive business impact. By identifying any patterns or relationships between variables, businesses can make more informed decisions and improve their operations."],"metadata":{"id":"6IcbL-yanx5r"}},{"cell_type":"markdown","source":["<h1><b>Feature Engineering & Data Pre-processing\n","\n"],"metadata":{"id":"eoziEcGia2W4"}},{"cell_type":"code","source":["#checking Outliers in numeric features using seaborn boxplot\n","\n","plt.figure(figsize=(20,15))\n","\n","for i,feature in enumerate(num_col):\n","  plt.subplot(3,3,i+1)\n","  sns.boxplot(df[feature])\n","  plt.title(feature, fontsize=16,color='red')\n","  plt.tight_layout()"],"metadata":{"id":"4kUXoRq8a8Vb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">It is not always necessary to remove outliers from a dataset, especially if the data is continuous and represents real-world measurements.\n","\n",">In the case of rainfall and snowfall data, if the outliers are due to extreme weather events or other natural phenomena, they may represent important information that should not be removed.\n","\n"],"metadata":{"id":"8Ee_s1_QbJeZ"}},{"cell_type":"markdown","source":["<h1><b>Check Correlation and Multicollinearity between features"],"metadata":{"id":"pfW2q-o-bTaV"}},{"cell_type":"code","source":["\n","#checking correlation between independent features using heatmap\n","\n","plt.figure(figsize=(15,6))\n","sns.heatmap(df.corr(),cmap='PiYG',annot=True)\n","plt.title('Correlation between all the variables', size=16,color='red')\n","plt.show()"],"metadata":{"id":"_TYqIHfhbYtI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">The correlation heatmap indicates a high positive correlation of 0.91 between 'Temperature' and 'Dew point temperature', suggesting that dropping one of the columns would not significantly affect our analysis. Therefore, we can drop the 'Dew point temperature(°C)' column since it has the same variation as 'Temperature'.\n","\n",">To ensure there is no collinearity between other variables, we will check their VIF values before proceeding further.\n","\n","\n","\n","'''<h3>What is Variance Inflation Factor (VIF)?\n","\n",">VIF determines the strength of the correlation between the independent variables. It is predicted by taking a variable and regressing it against every other variable.The default VIF cutoff value is 5; only variables with a VIF less than 5 will be included in the model. In some cases VIF of less than 10 is also acceptable.\n","\n",">Here, we have performed the VIF calculations for the clarity about the correlation between the features. after that, we have dropped the features which were highly correlated with any other independent features for accurate predictions.'''"],"metadata":{"id":"33Du0eDDbiuC"}},{"cell_type":"code","source":["# Function to calculate Multicollinearity\n","\n","# Checking the VIF\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","def calc_vif(X):\n","  # VIF dataframe\n","  vif_data = pd.DataFrame()\n","  vif_data[\"feature\"] = X.columns\n","\n","  # calculating VIF for each feature\n","  vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n","                          for i in range(len(X.columns))]\n","  vif_data['VIF'] = round(vif_data['VIF'],2)\n","  return(vif_data)"],"metadata":{"id":"Yqvrb8CDb6et"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["calc_vif(df[[i for i in df.describe().columns]])"],"metadata":{"id":"ffE2OxYbb6yk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">From heatmap and VIF, we can clearly visible Temperature and Dew point temperature(°C) has the high correlation and high multicollinearity respectively. As a result, to reduce correlation and multicollinearity We can drop dew point temperature column."],"metadata":{"id":"NSGWOlcK9ZyD"}},{"cell_type":"code","source":["# Drop Dew Point Temperature Column\n","\n","df.drop(columns= ['dew_point_temp'], inplace=True)"],"metadata":{"id":"spGlb82Nb-eh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Again plot correlation using heatmap\n","\n","plt.figure(figsize=(15,6))\n","sns.heatmap(df.corr(),cmap='PiYG',annot=True)\n","plt.title('Correlation between all the Variables', size=16, color='red')\n","plt.show()"],"metadata":{"id":"-e0oZ2PMcena"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h3><b> Why did you choose this specific chart\n","\n",">Correlation analysis is a statistical technique used to measure the strength and direction of the relationship between two or more variables. It is used to determine whether and how strongly two variables are related to each other."],"metadata":{"id":"FesxlkZjcroy"}},{"cell_type":"markdown","source":[" <h3><b>What is/are the insight(s) found from the chart?\n","\n"," >After removable of dew point temperature column, correlation between dependent variable and multicollinearity could be controlled."],"metadata":{"id":"d5Zx8LW1dAO4"}},{"cell_type":"markdown","source":["<h3><b>Convert unappropriate columns to appropriate columns for Machine learning"],"metadata":{"id":"kQ7cNlKROIkb"}},{"cell_type":"code","source":["#Changing the int64 column into category column\n","\n","cols=['hour','month','weekdays_weekend']\n","for col in cols:\n","  df[col]= df[col].astype('category')"],"metadata":{"id":"ZlejItA4ONNh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<B>Converting snowfall, rainfall and visibility to categorical attributes:"],"metadata":{"id":"jjYuDzV3PGNt"}},{"cell_type":"code","source":["# Converting snowfall and rainfall to categorical attributes\n","\n","df['snowfall'] = df['snowfall'].apply(lambda x: 1 if x>0 else 0)\n","df['rainfall'] = df['rainfall'].apply(lambda x: 1 if x>0 else 0)"],"metadata":{"id":"GzGzI_MMO_6t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h>Converting visibility to a categorical attribute:\n","\n","1. pd.cut() is a function from the pandas library that is used to segment data\n","   into bins. It takes the column df.visibility as the input data to be binned.\n","\n","2. The bins parameter specifies the bin edges or intervals. In this case, the\n","   bins are defined as [0, 399, 999, 2001]. This means that the values in the 'visibility' column will be divided into three bins: values less than or equal to 399, values greater than 399 and less than or equal to 999, and values greater than 999 and less than or equal to 2001.\n","\n","3. The labels parameter specifies the labels to assign to each bin. In this\n","   case, the labels are [0, 1, 2]. So, the first bin (values <= 399) will be labeled as 0, the second bin (values > 399 and <= 999) will be labeled as 1, and the third bin (values > 999 and <= 2001) will be labeled as 2.\n","\n","4. The resulting bin labels are assigned to the 'visibility' column of the\n","   DataFrame, creating a new column with the name 'visibility' and the binned values."],"metadata":{"id":"HNSHAjjYQK1Y"}},{"cell_type":"code","source":["# Binning the 'visibility' column into 3 categories using Pandas cut() function.\n","\n","df['visibility'] = pd.cut(df.visibility,bins=[0,399,999,2001],labels=[0,1,2])"],"metadata":{"id":"qn0g8ryxPP3Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Converting categorical columns to numerical using numpy's where() function.\n","\n","df['func_day'] = np.where(df['func_day'] == 'Yes',1,0)\n","df['holiday'] = np.where(df['holiday'] == 'Holiday', 1,0)"],"metadata":{"id":"xmQ5ZhEAQ505"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h3>In Data 'month', 'day_of_week', 'hour' are nominal categorical variables, therefore we have to apply onehot encoding."],"metadata":{"id":"LvwPUZrsREHQ"}},{"cell_type":"code","source":["# Converting categorical to float and integer\n","\n","df['visibility'] = df['visibility'].astype(float)\n","df['weekdays_weekend'] = df['weekdays_weekend'].astype(int)"],"metadata":{"id":"0MQ_b4eaRDBO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<H> Apply one hot encoding to convert data into numerical format"],"metadata":{"id":"cYskyxutRpJt"}},{"cell_type":"code","source":["# One-Hot Encoding of Categorical Features in Dataset\n","\n","df= pd.get_dummies(df,columns = ['month','hour','seasons'],drop_first=True)"],"metadata":{"id":"qsSqmRlxRDLP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h> convert the data into dependent and independent variable"],"metadata":{"id":"eqlG_6mSSHEN"}},{"cell_type":"code","source":["# Assigning the value of independent variable (X) and dependent variable (Y) :\n","\n","X = df.drop(columns=['rented_bike_count'], axis=1)\n","y = np.sqrt(df['rented_bike_count'])"],"metadata":{"id":"PETJuEfqRgX6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X.head()"],"metadata":{"id":"NAUC3l80Rgje"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y.head()"],"metadata":{"id":"KIZK19XKRgmv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h1><B>Data Spliting\n","\n","\n","<h2> why we use data spliting<h>\n","\n",">Dividing the data into training and testing sets is a common approach in machine learning to evaluate the performance of a model. The idea is to use the training data to estimate the parameters of the model, and the testing data to evaluate the performance of the model on new, unseen data.\n","\n",">By dividing the data into an 80/20 ratio, you are following the Pareto principle, which states that 80% of the effects come from 20% of the causes. In this case, the 80% of the data is used for training, and 20% is used for testing. This split ensures that you have enough data to accurately estimate the parameters of the model while also having enough data to accurately evaluate its performance.\n","\n",">However, it's important to note that the choice of split ratio (80/20 or any other) depends on the size of your dataset and the complexity of your model. If you have a large dataset, you may be able to use a smaller ratio (e.g., 70/30), while if you have a small dataset, you may need to use a larger ratio (e.g., 90/10).\n","\n",">In general, the goal is to find the right balance between the variance of the parameter estimates and the variance of the performance statistics, so that neither is too high. Therefore, I choose 70:30 ratio.\n"],"metadata":{"id":"PQrTOm3QNl1M"}},{"cell_type":"code","source":["# Dividing the dataset into train and test set\n","\n","from sklearn.model_selection import train_test_split\n","X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=0)"],"metadata":{"id":"KXIiwI6dRZoj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train"],"metadata":{"id":"-VZkkAH9Nkoz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h1><b> Data Scaling<h1>\n","\n","<h2><b>Which method have you used to scale you data and why?\n","\n","<h3>IN this method we use Min Max Scaling to Scale the data\n","\n","> I used MinMaxscaler as it preserves the shape of the original distribution.\n","  Note that MinMaxScaler doesn't reduce the importance of outliers. The default range for the feature returned by MinMaxScaler is 0 to 1."],"metadata":{"id":"JG_yZaukTwwr"}},{"cell_type":"code","source":["# Scaling your data\n","from sklearn.preprocessing import MinMaxScaler\n","\n","scaler = MinMaxScaler()\n","\n","# fit the scaler to the train set, it will learn the parameters\n","scaler.fit(X_train)\n","\n","# transform train and test sets\n","X_train_scaled = scaler.transform(X_train)\n","X_test_scaled = scaler.transform(X_test)"],"metadata":{"id":"dTR1A_0FTrZw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n","X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)"],"metadata":{"id":"dmmztA3-Trjk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h1><b>ML Model Implementation\n","\n","<b>First we apply linear regression"],"metadata":{"id":"SVRXpcNiVXaK"}},{"cell_type":"code","source":["# initalizing the model\n","regg = LinearRegression().fit(X_train,y_train)"],"metadata":{"id":"5kuUYureUr-W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Predicted Train & Test values\n","\n","y_pred_train = regg.predict(X_train)\n","y_pred_test = regg.predict(X_test)"],"metadata":{"id":"jqS71jlrZ9S1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking score\n","\n","regg.score(X_train,y_train)"],"metadata":{"id":"uOtLT8d2Z9ZG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Checking Coefficent\n","\n","regg.coef_"],"metadata":{"id":"iI64ldoPaEwV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Performance Metrics calculation function\n","\n","def print_metrics(actual, predicted):\n","  print('MSE is: {}'.format(mean_squared_error(actual, predicted)))\n","  print('RMSE is: {}'.format(math.sqrt(mean_squared_error(actual, predicted))))\n","  print('R2 score: is {}'.format(r2_score(actual, predicted)))\n","  print('MAE is: {}'.format(mean_absolute_error(actual, predicted)))\n","\n","\n","\n","# Here is the all info about how this function work\n","\n","'''MSE = mean_squared_error((y_train), (y_pred_train))\n","RMSE = np.sqrt(MSE)\n","MAE = mean_absolute_error((y_train), (y_pred_train))\n","R2 = r2_score((y_train), (y_pred_train))\n","Adj_R2 = (1-(1-r2_score(y_train, y_pred_train))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))'''"],"metadata":{"id":"1CtMvpSHaE0j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print_metrics(y_train, y_pred_train)"],"metadata":{"id":"NB4A0hAIaFDx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Adjusted R2 (considers and tests different independent variables against the model)\n","def Adjusted_R2(actual, predicted):\n","  Adj_R2 = (1-(1-r2_score(actual, predicted))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))\n","  return('Adjusted R2 :', Adj_R2)"],"metadata":{"id":"OzriVYd8bShd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Adjusted_R2(y_train, y_pred_train)"],"metadata":{"id":"tcra-Bq9bSk0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print_metrics(y_test, y_pred_test)\n","Adjusted_R2(y_test, y_pred_test)"],"metadata":{"id":"DKjcF_yxcdnW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">The linear regression model has a MSE of 32.93, which means that on average, the predicted values differ from the actual values by 5.74 (RMSE) units in the original units of the target variable.\n","\n",">The R2 score of 0.79 indicates that the model explains around 79% of the variance in the target variable, which is a good fit.\n","\n",">The MAE of 4.44 indicates that the average absolute difference between the predicted and actual values is 4.44 units in the original units of the target variable.\n","\n",">The adjusted R2 score of 0.78 suggests that the model is not overfitting, taking into account the number of features in the model.\n","\n",">Overall, these results suggest that the linear regression model is a good fit for the data and can be used to make predictions on new data."],"metadata":{"id":"h-N8Actielij"}},{"cell_type":"code","source":["# If our model is perfect, residuals would all be zeros\n","\n","test_residuals = y_test - y_pred_test\n","test_residuals"],"metadata":{"id":"ADY_C_QOcdrj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# residual plot\n","\n","sns.scatterplot(x=y_test, y=test_residuals)\n","plt.axhline(y=0, color='r', ls='--');"],"metadata":{"id":"BZMUhd8xcd2J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check Normal probability plot\n","\n","sns.displot(test_residuals, bins=50, kde=True);"],"metadata":{"id":"r34_sIg7jmW_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import scipy as sp\n","\n","# Create a figure and axis to plot on\n","fig, ax = plt.subplots(figsize=(6,8),dpi=100)\n","# probplot returns the raw values if needed\n","_ = sp.stats.probplot(test_residuals,dist='norm',plot=ax)"],"metadata":{"id":"zFjzIJWJED3L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot between actual target variable vs Predicted one\n","\n","plt.figure(figsize=(18,6))\n","plt.plot(y_pred_train[:250], color='r')\n","plt.plot(np.array(y_test)[:250], color='g')\n","plt.legend([\"Predicted\",\"Actual\"])\n","plt.show()"],"metadata":{"id":"MJ15jarTjmZ-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Checking Heteroscadacity\n","\n","plt.figure(figsize=(8,6))\n","plt.scatter((y_pred_test),(y_test)-(y_pred_test),marker='x')\n","plt.title('Heteroscadacity of Linear model')"],"metadata":{"id":"pZDROEnkjmeI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h><B> Observation\n","\n","> From residual and normal Probability plot, it is clearly visible our model is not perfect. We have to go for next level."],"metadata":{"id":"mNEQCsamj46P"}},{"cell_type":"markdown","source":["<h2><b>Apply Polnomial Regression"],"metadata":{"id":"9tmJTo0Mka24"}},{"cell_type":"code","source":["# importing polynominal features from sklearn\n","from sklearn.preprocessing import PolynomialFeatures\n","\n","# polynomial convertion\n","polynomial_converter = PolynomialFeatures(degree=2,include_bias=False)"],"metadata":{"id":"1-yG7xYzklh7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["poly_features = polynomial_converter.fit_transform(X)"],"metadata":{"id":"32-DDAT4kllg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.linear_model import LinearRegression\n","\n","model = LinearRegression(fit_intercept=True)\n","model.fit(X_train,y_train)\n","test_predictions = model.predict(X_test)"],"metadata":{"id":"QQ3BAlN6m_8f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print_metrics(y_train, y_pred_train)\n","\n","Adjusted_R2(y_test, y_pred_test)"],"metadata":{"id":"BCUq4YsBm_-3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> The polynomial linear regression model has a slightly lower MSE (32.80) and RMSE (5.73) than the linear regression model, indicating that it has a better fit.\n","\n",">The R2 score of 0.79 indicates that the model explains around 79% of the variance in the target variable, which is a good fit.\n","\n",">The MAE of 4.39 is also slightly lower than the MAE of the linear regression model, indicating that the average absolute difference between the predicted and actual values is slightly smaller.\n","\n",">The adjusted R2 score is the same as the linear regression model (0.78), suggesting that the polynomial model is not overfitting.\n","\n",">Overall, these results suggest that the polynomial linear regression model is a better fit for the data than the linear regression model and can be used to make more accurate predictions on new data."],"metadata":{"id":"mCzqJUVpnflH"}},{"cell_type":"markdown","source":["<h1><B> Apply Regularized Regression\n","\n","\n","<h6>Regularization attempts to minimize the RSS (residual sum of squares) and a penalty factor. This penalty factor will penalize models that have coefficients that are too large.<h>\n","\n","\n","\n","<h2>First we apply Ridge Regression(L2 Regularization)"],"metadata":{"id":"QED3uJLJnztB"}},{"cell_type":"code","source":["# Initalizing ridge regression\n","\n","ridge = Ridge(alpha = 0.1)\n","\n","ridge.fit(X_train,y_train)"],"metadata":{"id":"67h7Lqz3oBa1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''#checking score\n","\n","ridge.score(X_train,y_train)'''\n","\n","# Predicted Train & Test values\n","\n","y_pred_train_ridge=ridge.predict(X_train)\n","y_pred_test_ridge=ridge.predict(X_test)"],"metadata":{"id":"ezmAFjZuoBqJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#checking score\n","\n","ridge.score(X_train,y_train)"],"metadata":{"id":"PzztwdDPpX2B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h2><b>Choosing an Alpha value with cross-validation"],"metadata":{"id":"8L6qmuB4P7IH"}},{"cell_type":"code","source":["from sklearn.linear_model import RidgeCV\n","\n","ridge_cv_model = RidgeCV(alphas=(0.1, 1.0, 10.0),scoring='neg_mean_absolute_error',cv=5)"],"metadata":{"id":"UkuRzXz5s4oZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ridge_cv_model.fit(X_train,y_train)\n","ridge_cv_model.alpha_"],"metadata":{"id":"av_fLP6fQL9w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_predictions = ridge_cv_model.predict(X_test)"],"metadata":{"id":"yGvGN3y1QMBQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print_metrics(y_test, test_predictions)\n","Adjusted_R2(y_test, test_predictions)"],"metadata":{"id":"5O0ZIjAAQyLu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot between actual target variable vs Predicted one\n","\n","plt.figure(figsize=(18,6))\n","plt.plot(test_predictions[:250], color='r')\n","plt.plot(np.array(y_test)[:250], color='g')\n","plt.legend([\"Predicted\",\"Actual\"])\n","plt.show()"],"metadata":{"id":"WNCM7G-7QyPN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">The ridge regression model has similar results to the linear regression model, with a slightly higher MSE (32.93) and RMSE (5.74) but a similar R2 score of 0.79. The MAE of 4.44 is also similar to the linear regression model. The adjusted R2 score is slightly lower than the linear regression model (0.78), which suggests that the ridge regression model may be slightly overfitting.\n","\n",">Overall, these results suggest that the ridge regression model has a similar performance to the linear regression model but may not provide any significant improvement. However, ridge regression can be useful in reducing the impact of multicollinearity among the predictor variables and may improve the stability of the model.*italicised text*"],"metadata":{"id":"h_A6UB7CRNsd"}},{"cell_type":"markdown","source":["<h1><b>Lasso Regression(L1 Regularization) along with cross-validation"],"metadata":{"id":"hCh5MIy1RgId"}},{"cell_type":"code","source":["from sklearn.linear_model import LassoCV"],"metadata":{"id":"eeUTIlDDRXs2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initalizing lasso regression\n","\n","lasso_cv_model = LassoCV(eps=0.001,n_alphas=100,cv=3,max_iter=1000000)\n","lasso_cv_model.fit(X_train, y_train)"],"metadata":{"id":"BdFPvCD2RwmL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating the model score\n","\n","print(lasso_cv_model.score(X_test, y_test))\n","print(lasso_cv_model.score(X_train, y_train))"],"metadata":{"id":"dZl6NP3hRwqV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lasso_cv_model.fit(X_train,y_train)"],"metadata":{"id":"gayD0Z0fRwz7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lasso_cv_model.alpha_"],"metadata":{"id":"uppOqDHYR8z1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_predictions = lasso_cv_model.predict(X_test)"],"metadata":{"id":"xS0LFxTZR83h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate the performance metrics\n","\n","print_metrics(y_test, test_predictions)\n","Adjusted_R2(y_test, test_predictions)"],"metadata":{"id":"EqWCC4EoSDWr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Predicted Train & Test values\n","\n","y_pred_train_lasso=lasso_cv_model.predict(X_train)\n","y_pred_test_lasso=lasso_cv_model.predict(X_test)"],"metadata":{"id":"d-1FMUI9SDdC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot between actual target variable vs Predicted one\n","plt.figure(figsize=(18,6))\n","plt.plot(test_predictions[:250], color='r')\n","plt.plot(np.array(y_test)[:250], color='g')\n","plt.legend([\"Predicted\",\"Actual\"])\n","plt.show()"],"metadata":{"id":"Z7Q5AnQLSIzD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">The lasso regression model has a higher MSE (37.43) and RMSE (6.12) compared to both the linear regression and ridge regression models, indicating that it has a worse fit. The R2 score of 0.76 also suggests that the model explains a lower proportion of the variance in the target variable compared to the other models. The MAE of 4.79 is higher than the linear regression model and similar to the ridge regression model. The adjusted R2 score is also lower than the other models (0.75), indicating that the lasso regression model may be overfitting.\n","\n",">Overall, these results suggest that the lasso regression model does not perform as well as the other models in predicting the target variable. Lasso regression can be useful in selecting a subset of predictor variables and reducing the complexity of the model, but in this case, it seems that the full set of predictor variables may be necessary for a better fit."],"metadata":{"id":"cKpkW9IhSPFf"}},{"cell_type":"markdown","source":["<h1><b>Decision Tree Regression"],"metadata":{"id":"ld7C6NNeSite"}},{"cell_type":"code","source":["#Initilazing the model\n","\n","from sklearn.tree import DecisionTreeRegressor\n","dt_regressor = DecisionTreeRegressor(criterion='squared_error', max_depth=8, max_features=9, max_leaf_nodes=100)"],"metadata":{"id":"6VEFYczgSroF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dt_regressor.fit(X_train,y_train)"],"metadata":{"id":"vn0BizppSrzD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Train Test values\n","y_pred_train_d = dt_regressor.predict(X_train)\n","y_pred_test_d = dt_regressor.predict(X_test)"],"metadata":{"id":"h26B3sSxS2QX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculating Performance Metrics for training data\n","print_metrics((y_train), (y_pred_train_d))"],"metadata":{"id":"gt74wQGKS2Sz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculating Performance Metrics for Test data\n","print_metrics((y_test), (y_pred_test_d))"],"metadata":{"id":"O_x7YO37S2XW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#adjusted R2 score\n","Adjusted_R2((y_train), (y_pred_train_d))"],"metadata":{"id":"fIlCYwkrS9eW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot between actual target variable vs Predicted one\n","plt.figure(figsize=(18,6))\n","plt.plot(test_predictions[:250], color='r')\n","plt.plot(np.array(y_test)[:250], color='g')\n","plt.legend([\"Predicted\",\"Actual\"])\n","plt.show()"],"metadata":{"id":"p6pnBzChS9iC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">The decision tree model has a moderate fit, with a higher MSE and RMSE compared to linear regression models, but lower MAE than some other models.\n","\n",">The adjusted R2 score is the highest among all models, suggesting it may not be overfitting. Decision trees can capture nonlinear relationships, but are prone to overfitting and may not generalize well.\n","\n",">Overall, while it has some strengths, other models may be more suitable for this dataset."],"metadata":{"id":"Yspo4QvQTHGi"}},{"cell_type":"markdown","source":["<h1><b>Random Forest Regression"],"metadata":{"id":"i1YtMdZsTZwN"}},{"cell_type":"code","source":["# Initalizing the Model\n","rf_model = RandomForestRegressor(random_state=0)\n","parameters = {'n_estimators':[500],\n","             'min_samples_leaf':np.arange(25,31)\n","             }"],"metadata":{"id":"NDhPfI_ITfjE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rf_model.fit(X_train, y_train)"],"metadata":{"id":"NEgsqIc_TjHm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Train test values\n","y_pred_train_rf = rf_model.predict(X_train)\n","y_pred_test_rf = rf_model.predict(X_test)"],"metadata":{"id":"V_Vb8ATfTjRW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Calculating Performance Metrics for train data\n","print_metrics((y_train), (y_pred_train_rf))"],"metadata":{"id":"2ce5zCQMTjef"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#adjusted R2 score\n","Adjusted_R2((y_train), (y_pred_train_rf))"],"metadata":{"id":"zq6sbQSITjpA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculating Performance Metrics for Test data\n","print_metrics((y_test), (y_pred_test_rf))"],"metadata":{"id":"9H1U8l6OTwpO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#adjusted R2 score\n","Adjusted_R2((y_test), (y_pred_test_rf))"],"metadata":{"id":"ISmXxZmlTzaw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Feature importances\n","rf_model.feature_importances_"],"metadata":{"id":"tW_oXyepT4jz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["importances = rf_model.feature_importances_\n","\n","importance_dict = {'Feature' : list(X_train_scaled.columns),\n","                   'Feature Importance' : importances}\n","\n","importance_df = pd.DataFrame(importance_dict)\n","importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)"],"metadata":{"id":"PF9S9jYeT4ob"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# features = X_train_scaled.columns\n","# importances = rf_model.feature_importances_\n","# indices = np.argsort(importances)"],"metadata":{"id":"4tfRuGXuT9AF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.style.use('dark_background')"],"metadata":{"id":"gnfoJWGWT9IL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Feature importances\n","\n","rf_feat_imp = pd.Series(rf_model.feature_importances_, index=X.columns)\n","plt.figure(figsize=(15,5),dpi=200)\n","plt.title('Feature Importances: RANDOM FORESTS',fontsize=16,color='red')\n","plt.xlabel('Relative Importance')\n","rf_feat_imp.nlargest(20).plot(kind='barh', color='r')"],"metadata":{"id":"dQRLU7yTUNXM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">The random forest regression model has the lowest MSE, RMSE, and MAE compared to all other models, indicating the best fit.\n","\n",">The R2 score is high, indicating it explains a high proportion of the variance. The adjusted R2 score suggests it is not overfitting.\n","\n",">The random forest model is powerful and can handle nonlinear relationships, interactions, and high-dimensional data.\n","\n",">Overall, it performs well in predicting the target variable and is a good choice for this dataset."],"metadata":{"id":"Bo9EdbFLURVM"}},{"cell_type":"markdown","source":["<h1><b>XG Boost Regression"],"metadata":{"id":"GyJiQO5yUkju"}},{"cell_type":"code","source":["# Initializing the model\n","\n","xgb_r = xgb.XGBRegressor()"],"metadata":{"id":"2KtlREGJUn3E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Fitting the model\n","xgb_r.fit(X_train, y_train)"],"metadata":{"id":"ZTpoDs9sUwCn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Train Test values\n","y_pred_train_xgb = xgb_r.predict(X_train)\n","y_pred_test_xgb = xgb_r.predict(X_test)"],"metadata":{"id":"X8CrK8VGUwGg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculating Performance Metrics for train data\n","print_metrics((y_train), (y_pred_train_xgb))"],"metadata":{"id":"Ce_3mjF_UwJU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#adjusted R2 score\n","Adjusted_R2((y_train), (y_pred_train_xgb))"],"metadata":{"id":"cn7_dT-YVArT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculating Performance Metrics for Test data\n","print_metrics((y_test), (y_pred_test_xgb))"],"metadata":{"id":"mebfm_v0VAuA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#adjusted R2 score\n","Adjusted_R2((y_test), (y_pred_test_xgb))"],"metadata":{"id":"gJGALXdHVAxs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Feature importances\n","xgb_r.feature_importances_"],"metadata":{"id":"TBq4_4v1VLh0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["features = X_train.columns\n","importances = xgb_r.feature_importances_\n","indices = np.argsort(importances)"],"metadata":{"id":"hxWN4J9-VLkL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Feature importances\n","rf_feat_imp = pd.Series(xgb_r.feature_importances_, index=X.columns)\n","plt.figure(figsize=(15,5),dpi=200)\n","plt.title('Feature Importances: XG Boost regression',fontsize=16,color='red')\n","plt.xlabel('Relative Importance')\n","rf_feat_imp.nlargest(20).plot(kind='barh', color='r')\n","plt.show()"],"metadata":{"id":"368m8F7pVLoB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">The XGBoost regression model has a lower MSE, RMSE, and MAE than most models, suggesting a good fit. The R2 score of 0.86 indicates that it explains a high proportion of the variance in the target variable. The adjusted R2 score of 0.86 suggests that it is not overfitting.\n","\n",">XGBoost is a powerful algorithm that can handle nonlinear relationships and interactions in the data, and is known for its high predictive accuracy. Overall, these results suggest that the XGBoost model is a good choice for this dataset, as it performs well in predicting the target variable."],"metadata":{"id":"7ObhRCEQVWxF"}},{"cell_type":"markdown","source":["<h1><b>Hyperparameter Tuning :\n","\n","<h2>XG Boost Regressor with GridSearchCV"],"metadata":{"id":"H-hOO3tuVjwy"}},{"cell_type":"code","source":["# Number of trees\n","n_estimators = [50,80,100]\n","\n","# Maximum depth of trees\n","max_depth = [4,6,8]\n","\n","# Minimum number of samples required to split a node\n","min_samples_split = [50,100,150]\n","\n","# Minimum number of samples required at each leaf node\n","min_samples_leaf = [40,50]\n","\n","# HYperparameter Grid\n","parameter_dict = {'n_estimators' : n_estimators,\n","              'max_depth' : max_depth,\n","              'min_samples_split' : min_samples_split,\n","              'min_samples_leaf' : min_samples_leaf}"],"metadata":{"id":"2-vWXd29VwcB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["parameter_dict"],"metadata":{"id":"zW6Ki6EXWO5o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create an instance of the XG Boost Regressor\n","xg_boost = xgb.XGBRegressor()\n","\n","# Grid search\n","xg_grid = GridSearchCV(estimator=xg_boost,\n","                       param_grid = parameter_dict,\n","                       cv = 5, verbose=2)\n","\n","xg_grid.fit(X_train,y_train)"],"metadata":{"id":"-I9F-j3IWPEu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["xg_grid.best_estimator_"],"metadata":{"id":"QKA3qWwsWVeQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["xg_optimal_model = xg_grid.best_estimator_"],"metadata":{"id":"PxydRjfuWVnZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Train Test values\n","y_pred_train_xg_opt = xg_optimal_model.predict(X_train)\n","y_pred_test_xg_opt= xg_optimal_model.predict(X_test)"],"metadata":{"id":"-Vk4zDEXWckH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculating Performance Metrics for train data\n","print_metrics((y_train), (y_pred_train_xg_opt))"],"metadata":{"id":"LlXCr-JmWcsk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# adjusted R2 score\n","Adjusted_R2((y_train), (y_pred_train_xg_opt))"],"metadata":{"id":"q0odtm6EWh_G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculating Performance Metrics for test data\n","print_metrics((y_test), (y_pred_test_xg_opt))"],"metadata":{"id":"3Rkv0uLIWiIY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#adjusted R2 score\n","Adjusted_R2((y_test), (y_pred_test_xg_opt))"],"metadata":{"id":"PIhc9H6GWnxq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["xg_optimal_model.feature_importances_"],"metadata":{"id":"oKxcTV2SWn6p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["xg_optimal_model.fit(X_train,y_train)"],"metadata":{"id":"iDWX4OzJWwdd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["features = X_train.columns\n","importance = xg_optimal_model.feature_importances_\n","index = np.argsort(importance)"],"metadata":{"id":"bAs9mrM5Wy67"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Feature importances\n","rf_feat_imp = pd.Series(xg_optimal_model.feature_importances_, index=X.columns)\n","plt.figure(figsize=(15,5),dpi=200)\n","plt.title('Feature Importances: XG Boost Regressor with GridSearchCV',fontsize=16,color='red')\n","plt.xlabel('Relative Importance')\n","rf_feat_imp.nlargest(20).plot(kind='barh', color='r')\n","plt.show()\n"],"metadata":{"id":"qPT6X06BW1l6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">The XGBoost Regressor with GridSearchCV has a lower MSE, RMSE, and MAE than most models, indicating a good fit. The R2 score of 0.91 suggests that it explains a high proportion of the variance in the target variable. The adjusted R2 score of 0.91 suggests that it is not overfitting.\n","\n",">Using GridSearchCV, the model is optimized by finding the best hyperparameters. This results in a more accurate and robust model. The XGBoost algorithm is powerful, and combining it with GridSearchCV helps to improve the performance of the model.\n","\n",">Overall, these results suggest that the XGBoost Regressor with GridSearchCV is a good choice for this dataset, as it performs well in predicting the target variable with high accuracy and robustness."],"metadata":{"id":"5qoj2RL4W5Su"}},{"cell_type":"markdown","source":["# **Conclusion**\n","\n","* The goal of this project is to develop a machine learning model that can accurately predict the demand for rental bikes based on different weather and other conditions.\n","\n","* The XG Boost prediction model had the lowest RMSE.\n","\n","* After applying several regression models to the dataset, it can be concluded that the XGBoost Regressor with GridSearchCV provides the best results with an MSE of 14.27, RMSE of 3.78, R2 score of 0.91, and MAE of 2.59. The model also has an adjusted R2 score of 0.91, indicating that it is not overfitting.\n","\n","* The Random Forest Regressor also showed promising results with an MSE of 15.95, RMSE of 3.99, R2 score of 0.89, and MAE of 2.64.\n","\n","* he XG Boost Regressor with GridSearchCV has a slightly lower MSE, RMSE, and MAE, and a higher R2 score and Adjusted R2 score than the Random Forest Regression model. This indicates that the XG Boost Regressor is the better model for predicting the target variable in this dataset. However, both models have shown promising results, and the final choice of model for deployment depends on the business need. If high accuracy in results is necessary, the XG Boost Regressor should be deployed. If the model interpretability is important to the stakeholders, then the Random Forest Regression model can be considered.\n","\n","* The Polynomial Linear Regression and Ridge Regression models also performed well, but with slightly higher error rates.\n","\n","* The Lasso Regression and Elastic Net Regression models did not perform as well, with higher error rates and lower R2 scores.\n","\n","* The Decision Tree Regressor showed relatively lower performance, with an MSE of 59.55, RMSE of 7.72, R2 score of 0.61, and MAE of 5.45.\n","\n","* Overall, the XGBoost Regressor with GridSearchCV is the most appropriate model for this dataset, as it provides high accuracy and robustness in predicting the target variable."],"metadata":{"id":"gCX9965dhzqZ"}},{"cell_type":"markdown","source":["### ***Hurrah! You have successfully completed your Supervised ML Capstone Project !!!***"],"metadata":{"id":"z0Mo2WYtJzUK"}}]}